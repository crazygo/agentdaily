[
  {
    "title": "Agents using CLI tools in place of REST APIs",
    "description": "Simon Willison discusses why AI agents are better off using CLI tools instead of REST APIs: to save on context window, improve accuracy and success rate when multiple tool calls are involved (particularly for pagination, rate-limit backoff, authentication failures), and lower the bar so cheap/fast models (gpt-5-nano, haiku-4.5) can reliably succeed. Using raw APIs is something only costly strong models (gpt-5.2, opus-4.5) can manage.",
    "author": "Simon Willison",
    "source": "Blog",
    "url": "https://simonwillison.net/2026/Jan/17/",
    "published_date": "2026-01-17",
    "topics": ["AI agents", "CLI tools", "API design", "context window", "model capabilities"],
    "type": "technical"
  },
  {
    "title": "Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems",
    "description": "Researchers question whether query-level workflow generation is always necessary for multi-agent systems. They show that a small set of top-K best task-level workflows already covers equivalent or more queries. They propose SCALE, a low-cost task-level generation framework that maintains competitive performance with just 0.61% average degradation while cutting token usage by up to 83%.",
    "author": "Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng",
    "source": "ArXiv",
    "url": "https://arxiv.org/abs/2601.11147",
    "published_date": "2026-01-16",
    "topics": ["multi-agent systems", "workflow optimization", "LLM efficiency", "token reduction", "agentic AI"],
    "type": "technical"
  },
  {
    "title": "When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models",
    "description": "Study investigates ensembling for unconditional score-based diffusion models. Finds that while ensembling improves score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics like FID on image datasets. Researchers provide theoretical insights into summing score models, with implications for ensembling and model composition techniques like guidance.",
    "author": "Raphaël Razafindralambo, Rémy Sun, Frédéric Precioso, Damien Garreau, Pierre-Alexandre Mattei",
    "source": "ArXiv",
    "url": "https://arxiv.org/abs/2601.11444",
    "published_date": "2026-01-16",
    "topics": ["diffusion models", "ensembling", "generative models", "FID", "model composition"],
    "type": "technical"
  },
  {
    "title": "Is More Context Always Better? Examining LLM Reasoning for Time Interval Prediction",
    "description": "Researchers at Amazon (Cao et al.) study LLM capabilities in predicting time intervals between recurring user actions. Key findings: LLMs surpass lightweight statistical baselines but consistently underperform dedicated ML models, showing limited ability to capture quantitative temporal structure. Moderate context improves accuracy, but adding further user-level detail degrades performance, challenging the assumption that 'more context leads to better reasoning'. Accepted at WWW 2026.",
    "author": "Yanan Cao, Farnaz Fallahi, et al.",
    "source": "ArXiv",
    "url": "https://arxiv.org/abs/2601.10132",
    "published_date": "2026-01-15",
    "topics": ["LLM reasoning", "context window", "temporal prediction", "model limitations"],
    "type": "technical"
  },
  {
    "title": "Open Responses: Vendor-neutral LLM API specification",
    "description": "Simon Willison highlights the Open Responses initiative, a vendor-neutral specification for the JSON API that clients can use to talk to hosted LLMs. Derived from OpenAI's Responses API, it includes launch partners like OpenRouter, Hugging Face, LM Studio, vLLM, Ollama, and Vercel. Simon emphasizes the need for comprehensive, language-independent conformance test suites for both servers and clients.",
    "author": "Simon Willison",
    "source": "Blog",
    "url": "https://simonwillison.net/2026/Jan/15/open-responses/",
    "published_date": "2026-01-15",
    "topics": ["Open Responses", "API standards", "LLM interoperability", "conformance testing"],
    "type": "opinion"
  },
  {
    "title": "Claude Cowork Exfiltrates Files via API Domain",
    "description": "Security researchers at Prompt Armor discovered a prompt injection vulnerability in Claude Cowork. By including an attacker's Anthropic API key in malicious content, they could trick the agent into uploading files it could see to api.anthropic.com/v1/files endpoint, bypassing outbound HTTP restrictions. This highlights the ongoing challenge of securing AI agents against prompt injection attacks.",
    "author": "Simon Willison",
    "source": "Blog",
    "url": "https://simonwillison.net/2026/Jan/14/claude-cowork-exfiltrates-files/",
    "published_date": "2026-01-14",
    "topics": ["security", "prompt injection", "AI agents", "data exfiltration", "Claude Cowork"],
    "type": "technical"
  },
  {
    "title": "LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities",
    "description": "Yongjian Tang and Thomas Runkler present a systematic review of LLM-based multi-agent systems across the Software Development Life Cycle (SDLC). The paper covers requirements engineering, code generation, static code checking, testing, and debugging. It identifies key challenges in multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. Accepted to GenSE 2026 workshop.",
    "author": "Yongjian Tang, Thomas Runkler",
    "source": "ArXiv",
    "url": "https://arxiv.org/abs/2601.09822",
    "published_date": "2026-01-14",
    "topics": ["LLM agents", "software engineering", "multi-agent systems", "SDLC", "research"],
    "type": "technical"
  },
  {
    "title": "Anthropic invests $1.5 million in Python Software Foundation",
    "description": "Anthropic announced a two-year partnership with the Python Software Foundation (PSF) contributing $1.5 million to support Python ecosystem security, including crucial security advances to CPython and PyPI, sustaining the PSF's core work supporting the Python language, ecosystem, and global community through the Developer in Residence program.",
    "author": "Simon Willison",
    "source": "Blog",
    "url": "https://simonwillison.net/2026/Jan/13/anthropic-python-foundation/",
    "published_date": "2026-01-13",
    "topics": ["Python", "open source", "security", "Anthropic", "PSF"],
    "type": "opinion"
  },
  {
    "title": "Show HN: Yolobox – Run AI coding agents with full sudo without nuking home dir",
    "description": "Finbarr's Yolobox project on Hacker News (Jan 13, 2026) addresses a critical security challenge: running AI coding agents with full sudo privileges without risking home directory destruction. The project generated significant community discussion (122 points, 99 comments) about the practical security concerns when giving coding agents elevated system access.",
    "author": "Finbarr",
    "source": "Hacker News",
    "url": "https://news.ycombinator.com/front?day=2026-01-13",
    "published_date": "2026-01-13",
    "topics": ["AI agents", "security", "sudo access", "sandboxing", "coding tools"],
    "type": "discussion"
  },
  {
    "title": "My LLM coding workflow going into 2026",
    "description": "Hacker News discussion from January 2026 about LLM-assisted coding workflows. Commenters express skepticism about whether extensive AI-assisted workflows actually save time compared to traditional coding. Discussion touches on 'vibe coding' - using AI with partial attention - and the trade-offs between detailed AI workflow planning versus simpler AI-assisted approaches. Some debate about whether AI actually provides speedups, with references to METR research showing limited AI productivity gains.",
    "author": "Hacker News Community",
    "source": "Hacker News",
    "url": "https://news.ycombinator.com/item?id=46489061",
    "published_date": "2026-01-13",
    "topics": ["LLM coding", "workflow", "productivity", "vibe coding", "AI skepticism"],
    "type": "discussion"
  }
]
