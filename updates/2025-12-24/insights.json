[
  {
    "title": "How AI coding agents work—and what to remember if you use them",
    "description": "Benj Edwards provides a comprehensive technical deep-dive into how AI coding agents function under the hood. Key technical insights: 1) Core architecture - supervising LLM interprets tasks and assigns them to parallel LLMs that use software tools; 2) Context problem - 'context rot' means models lose accuracy as token count increases, creating diminishing returns; 3) Tricks of the trade - context compression periodically summarizes history (losing details but preserving key decisions), and agents use external tools like Python scripts to avoid loading large files into context; 4) Multi-agent systems - orchestrator-worker pattern with lead agent coordinating specialized subagents, burning 15x more tokens than chat; 5) Best practices - human planning is crucial, with Anthropic recommending asking agents to read files and plan before coding; 6) Real-world performance - METR study found experienced developers took 19% longer with AI tools despite feeling faster. The article warns against 'vibe coding' and emphasizes human accountability.",
    "author": "Benj Edwards",
    "source": "TechMedia",
    "url": "https://arstechnica.com/ai/2025/12/how-do-ai-coding-agents-work-we-look-under-the-hood/",
    "published_date": "2025-12-24",
    "topics": ["AI Agents", "Claude Code", "Codex", "Context Windows", "Multi-Agent Systems", "LLM Architecture", "Best Practices"],
    "type": "technical"
  },
  {
    "title": "OpenAI says AI browsers may always be vulnerable to prompt injection attacks",
    "description": "OpenAI researchers found that AI-powered web browsing agents may always be vulnerable to prompt injection attacks hidden in webpage content, revealing fundamental security challenges for autonomous AI agents that navigate and interact with the web autonomously. The research highlights that even state-of-the-art models like GPT-5.1 can be tricked by malicious commands hidden in webpages, with implications for the future of AI agent security and reliability.",
    "author": "TechCrunch",
    "source": "TechMedia",
    "url": "https://techcrunch.com/2025/12/22/openai-says-ai-browsers-may-always-be-vulnerable-to-prompt-injection-attacks/",
    "published_date": "2025-12-22",
    "topics": ["AI Security", "Prompt Injection", "AI Agents", "OpenAI", "Web Browsing Agents", "GPT-5.1"],
    "type": "technical"
  },
  {
    "title": "2025 Was the Year AI Changed Software Engineering",
    "description": "YouTube video reviewing how 2025 was a paradigm shift year for AI in software engineering - moving from tab-tab-tab completion to building entire systems with a single prompt. The video covers how AI coding evolved from simple autocomplete to system-building capabilities, laying the foundation for future development practices.",
    "author": "Matt Maher",
    "source": "YouTube",
    "url": "https://www.youtube.com/watch?v=betoeAJnxcw",
    "published_date": "2025-12-22",
    "topics": ["AI", "Software Engineering", "AI Coding", "Paradigm Shift", "Vibe Coding"],
    "type": "opinion"
  },
  {
    "title": "Reflections on AI at the End of 2025 - Hacker News Discussion",
    "description": "Massive Hacker News discussion (348 comments) on antirez's blog post about AI in 2025. Key community themes: 1) Trust and verification - debate over whether LLM output is verifiable in coding vs other domains like medical advice; 2) LLMs as stochastic parrots - renewed debate on whether LLMs truly 'understand' or are just sophisticated pattern matchers; 3) Programmer resistance to AI - discussion on whether skepticism has decreased as tools improved; 4) RLVR (Reinforcement Learning from Verifiable Rewards) cited as major breakthrough for reasoning capabilities; 5) Economic concerns - worries about job displacement, AI company profitability, and 'enshittification'; 6) Safety concerns - mention of AI alignment and existential risk debates; 7) Practical usage - developers sharing both successes and failures with AI coding tools. The thread showcases the full spectrum of AI opinion in late 2025, from optimism to skepticism.",
    "author": "Hacker News Community",
    "source": "HN",
    "url": "https://news.ycombinator.com/item?id=46334819",
    "published_date": "2025-12-22",
    "topics": ["AI", "LLM", "Community Discussion", "AI Safety", "Job Market", "RLVR", "Stochastic Parrots"],
    "type": "discussion"
  },
  {
    "title": "A guide to local coding models - Hacker News Discussion",
    "description": "Hacker News discussion about local LLM coding experiences, comparing different models and approaches for running AI coding assistants locally.",
    "author": "Hacker News Community",
    "source": "HN",
    "url": "https://news.ycombinator.com/item?id=46348329",
    "published_date": "2025-12-22",
    "topics": ["Local LLMs", "AI Coding", "Code Models", "Privacy"],
    "type": "discussion"
  },
  {
    "title": "LLM Year in Review - Hacker News Discussion",
    "description": "Hacker News community discussion on Andrej Karpathy's 2025 LLM Year in Review, with insights on Cursor vs Claude Code, practical experiences with AI coding tools, and community perspectives on the paradigm shifts.",
    "author": "Hacker News Community",
    "source": "HN",
    "url": "https://news.ycombinator.com/item?id=46330726",
    "published_date": "2025-12-20",
    "topics": ["LLM", "Claude Code", "Cursor", "AI Coding", "Community Discussion"],
    "type": "discussion"
  },
  {
    "title": "2025 LLM Year in Review - 6 Major Paradigm Changes",
    "description": "Andrej Karpathy's comprehensive year-end review covering: 1) Reinforcement Learning from Verifiable Rewards (RLVR) as a new major training stage that enables reasoning capabilities through verifiable environments; 2) Ghosts vs Animals/Jagged Intelligence - understanding LLMs as fundamentally different entities with jagged capability profiles; 3) Cursor revealing a new layer of LLM apps that orchestrate multiple LLM calls with vertical-specific GUIs; 4) Claude Code as the first convincing LLM agent demonstration running locally on user computers; 5) Vibe Coding - AI crossing threshold to build impressive programs via English, making programming accessible to everyone; 6) Google Gemini Nano banana as early hint of LLM GUI moving beyond text chat to visual/spatial interfaces. Key insight: LLMs are emerging as new kind of intelligence, simultaneously smarter and dumber than expected, with industry having realized less than 10% of their potential.",
    "author": "Andrej Karpathy",
    "source": "Blog",
    "url": "https://karpathy.bearblog.dev/year-in-review-2025/",
    "published_date": "2025-12-19",
    "topics": ["LLM", "Reinforcement Learning", "AI Agents", "Vibe Coding", "Cursor", "Claude Code", "AI Paradigms", "Jagged Intelligence", "AI GUI"],
    "type": "opinion"
  },
  {
    "title": "We asked four AI coding agents to rebuild Minesweeper—the results were explosive",
    "description": "Ars Technica conducted a practical test of four AI coding agents (OpenAI Codex GPT-5, Anthropic Claude Code Opus 4.5, Google Gemini CLI, Mistral Vibe) by asking them to recreate Minesweeper with sound effects and a fun feature. Results: 1) OpenAI Codex won (9/10) - fastest implementation, included crucial 'chording' feature, good mobile support, but 'Lucky Sweep Bonus' was unbalanced; 2) Claude Code (7/10) - fastest coding speed (under 5 minutes), most polished presentation with Power Mode, but missing chording feature; 3) Mistral Vibe (4/10) - slow but functional for open-weight model, missing chording and sound effects; 4) Google Gemini CLI (0/10) - complete failure, took hours, couldn't produce working code even after two attempts. Key insight: AI agents currently work best as interactive tools augmenting human skill rather than replacements, with capabilities varying widely between models.",
    "author": "Kyle Orland and Benj Edwards",
    "source": "TechMedia",
    "url": "https://arstechnica.com/ai/2025/12/the-ars-technica-ai-coding-agent-test-minesweeper-edition/",
    "published_date": "2025-12-19",
    "topics": ["AI Agents", "Claude Code", "Codex", "Gemini", "Mistral", "Benchmarking", "Practical Testing"],
    "type": "technical"
  },
  {
    "title": "Your job is to deliver code you have proven to work",
    "description": "Simon Willison argues that AI-assisted developers must deliver proven, tested code rather than dumping untested PRs on reviewers. He outlines two essential steps: 1) Manual testing - developers must see their code work themselves, documenting the process with terminal commands or screen captures; 2) Automated testing - leveraging LLM tools to write tests that prove changes work. With the rise of coding agents like Claude Code, developers must learn to make agents prove their changes work through both manual and automated testing. The key insight: anyone can prompt an LLM to generate code, but value comes from contributing proven, working code - with humans providing accountability.",
    "author": "Simon Willison",
    "source": "Blog",
    "url": "https://simonwillison.net/2025/Dec/18/code-proven-to-work/",
    "published_date": "2025-12-18",
    "topics": ["AI Coding", "Code Testing", "Software Engineering", "Claude Code", "Coding Agents", "Best Practices"],
    "type": "opinion"
  },
  {
    "title": "Holistic Evaluation of State-of-the-Art LLMs for Code Generation",
    "description": "Comprehensive empirical evaluation of 6 state-of-the-art LLMs for code generation capabilities, assessing their performance across various programming tasks and benchmarks.",
    "author": "ArXiv Researchers",
    "source": "Paper",
    "url": "https://arxiv.org/abs/2512.18131",
    "published_date": "2025-12-18",
    "topics": ["LLM", "Code Generation", "Evaluation", "Benchmarking"],
    "type": "technical"
  }
]